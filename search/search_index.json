{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Batista Group Tutorials","text":"<p>Welcome to the internal knowledgebase for the Batista Group at Yale University. This resource serves as a centralized repository of information, protocols, tutorials, and documentation for group members.</p>"},{"location":"#about-us","title":"About Us","text":"<p>The Batista Group, led by Prof. Victor S. Batista at Yale University, focuses on cutting-edge research in theoretical and computational chemistry. Visit our official website to learn more about our research and team members.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#github-repositories","title":"GitHub Repositories","text":"<p>Our group maintains several open-source projects and research tools on GitHub. You can find our repositories at:</p> <ul> <li>Batista Lab GitHub Organization</li> </ul>"},{"location":"#conference-talks","title":"Conference Talks","text":"<ul> <li>Quantum Winter School 2026. Quantum Simulation for Chemistry and Materials Science. Part 1, Part 2</li> </ul>"},{"location":"#links-to-our-projects","title":"Links to our projects","text":"<ol> <li>DirectMultiStep - Direct Route Generation for Multi-Step Retrosynthesis.</li> <li>FragmentRetro - A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms</li> <li>QFlux - QFlux: An Open-Source Toolkit for Quantum Dynamics Simulations on Quantum Computers</li> <li>Models UI - a website to run some of our ML models directly from browser</li> </ol>"},{"location":"labels/","title":"Labels Guide","text":"<p>This document describes the labels used in our knowledgebase repository and their corresponding commit prefixes.</p>"},{"location":"labels/#labels","title":"Labels","text":""},{"location":"labels/#core-science","title":"Core Science","text":"Label Commit Prefix(es) <code>quantum-mechanics</code> <code>quantum:</code>, <code>qm:</code> <code>quantum-computing</code> <code>qc:</code> <code>quantum-dynamics</code> <code>qd:</code> <code>molecular-dynamics</code> <code>md:</code> <code>ai-ml</code> <code>ai:</code>, <code>ml:</code>"},{"location":"labels/#programming","title":"Programming","text":"Label Commit Prefix(es) <code>git</code> <code>git:</code> <code>python</code> <code>python:</code>, <code>py:</code>"},{"location":"labels/#educational","title":"Educational","text":"Label Commit Prefix(es) <code>tutorials</code> <code>tutorial:</code> <code>lectures</code> <code>lecture:</code>"},{"location":"labels/#publications","title":"Publications","text":"Label Commit Prefix(es) <code>publications</code> <code>pub:</code>"},{"location":"labels/#maintenance","title":"Maintenance","text":"Label Commit Prefix(es) <code>maintenance</code> <code>maint:</code>"},{"location":"labels/#version-impact","title":"Version Impact","text":"<p>Labels affect the version number of the next release as follows:</p>"},{"location":"labels/#major-version-bump","title":"Major Version Bump","text":"<ul> <li>Manual label <code>major</code> for major updates</li> </ul>"},{"location":"labels/#minor-version-bump","title":"Minor Version Bump","text":"<ul> <li>All core science labels</li> <li>Educational content labels</li> <li>Publications label</li> </ul>"},{"location":"labels/#patch-version-bump","title":"Patch Version Bump","text":"<ul> <li>Maintenance label</li> <li>Default for unlabeled changes</li> </ul>"},{"location":"git/github-workflows/","title":"GitHub Workflows: Automated CI/CD Pipeline Management","text":""},{"location":"git/github-workflows/#overview","title":"Overview","text":"<p>GitHub Workflows are automated processes that help you build, test, package, release, and deploy projects directly from your GitHub repository. They enable continuous integration and continuous deployment (CI/CD) through declarative YAML files stored in your <code>.github/workflows</code> directory.</p> <p>In the intro to git, a recommended workflow was making all changes on separate branches, which are merged into master through a Pull Request that is reviewed by the team. Workflows allows to automate many of the check the team might want to perform. For example, you'd want to know: is the code properly linted &amp; formatted? Is it type annotated? Does it pass all test cases? Because even if you recommend people to use pre-commit hooks, there's no guarantee that they're actually used. So you could have workflows to answer all of the questions above.</p>"},{"location":"git/github-workflows/#prerequisites","title":"Prerequisites","text":"<p>To work with GitHub Workflows, you'll need a GitHub repository and write access to configure workflows. You should also be familiar with basic Git operations and YAML syntax, as workflows are defined using YAML configuration files.</p>"},{"location":"git/github-workflows/#core-concepts","title":"Core Concepts","text":""},{"location":"git/github-workflows/#workflow-structure","title":"Workflow Structure","text":"<p>A GitHub workflow consists of:</p> <pre><code>name: Workflow Name           # The name that appears in GitHub Actions\non: [trigger_events]         # Events that trigger the workflow\njobs:                        # Groups of steps to execute\n  job_id:                    # Unique identifier for the job\n    runs-on: ubuntu-latest   # Runner environment\n    steps:                   # Individual tasks to perform\n      - name: Step Name      # Description of the step\n        uses: action/repo    # Action to use\n        with:                # Input parameters\n          param: value\n</code></pre>"},{"location":"git/github-workflows/#common-trigger-events","title":"Common Trigger Events","text":"<p>Workflows can be triggered by various events in your repository. The most common triggers include pushes to specific branches, pull request activities (opened, reopened, or synchronized), manual triggers through <code>workflow_dispatch</code>, and scheduled runs using cron syntax.</p>"},{"location":"git/github-workflows/#practical-examples","title":"Practical Examples","text":"<p>As an example, we'll go through workflows configured in our python-template repo.</p>"},{"location":"git/github-workflows/#code-quality","title":"Code Quality","text":"<p>The .github/workflows/quality.yml workflow proceeds as:</p> <ol> <li>Install the package with dev dependencies <code>uv pip install -e \".[dev]\"</code></li> <li>Check linting <code>ruff check</code></li> <li>Check formatting `ruff format --check``</li> <li>Check type annotations <code>mypy .</code> (this will follow the config in <code>pyproject.toml</code>, i.e. this will be a strict check if pyproject has <code>strict=True</code> flag)</li> </ol> <p>The particular implementation above caches the venv so that <code>uv pip install -e \".[dev]\"</code> doesn't have to redownload all dependencies each time the workflow is triggered. The quality workflow runs on any push and any pull request.</p>"},{"location":"git/github-workflows/#tests","title":"Tests","text":"<p>The .github/workflows/tests.yml workflow mirrors the <code>quality.yml</code> but instead of running ruff and mypy, it runs <code>pytest -v</code>. As written, tests workflow is also triggered on every push and pull request. Properly maintained codebases may have such extensive test suits that their execution takes awhile, which is why the tests workflow is only triggered upon a PR (and not every individual commit). To make <code>tests.yml</code> run only on pull requests, simply modify</p> <pre><code># replace this\non: [push, pull_request] \n# with\non: [pull_request] \n</code></pre>"},{"location":"git/github-workflows/#release-drafter","title":"Release Drafter","text":"<p>The Release Drafter automates changelog generation by categorizing pull requests based on their labels. It consists of two key components:</p> <ol> <li>Configuration (.github/release-drafter.yml):</li> </ol> <pre><code>name-template: v$RESOLVED_VERSION \ud83d\udcda      # Release name format\ncategories:                               # Organizes changes by category\n  - title: \u269b\ufe0f Core Science               # Scientific updates\n    labels:\n      - quantum-mechanics\n      - quantum-computing\n      - quantum-dynamics\n  - title: \ud83d\udcbb Programming &amp; Tools         # Development updates\n    labels:\n      - git\n      - python\n  - title: \ud83d\udcd6 Educational                # Learning materials\n    labels:\n      - tutorials\n      - lectures\n</code></pre> <ol> <li>Version Resolution (.github/workflows/release-drafter.yml):     The workflow employs a semantic versioning strategy based on PR labels. Major version bumps are triggered by breaking changes, minor versions by new features in areas like quantum mechanics or tutorials, and patch versions by maintenance updates. If no specific version label is present, the system defaults to a patch version increment.</li> </ol> <p>When a PR is opened or updated, the workflow analyzes commit messages for keywords, automatically assigns corresponding labels, uses these labels to categorize changes in the release draft, and updates the version number based on the labels' significance. For instance, a PR with commit message \"feat(quantum): Add new QM simulation\" would be labeled as \"quantum-mechanics\", appear under \"Core Science\" in the changelog, trigger a minor version bump, and be formatted as \"- Add new QM simulation @author (#123)\". To see existing labels for the knowledgebase repo, see labels doc.</p>"},{"location":"git/github-workflows/#update-major-minor-tags","title":"Update Major Minor Tags","text":"<p>The .github/workflows/update-major-minor-tags.yml workflow updates major/minor release tags on a tag push. e.g. update v1 and v1.2 tag when released v1.2.3.</p> <p>Tags accompany releases and allow you to reference particular commits (states of the repo) when that version was released. e.g. you can run <code>git checkout v1</code> (which will point to the latest version of v1 release) or <code>git checkout v1.2.3</code>.</p>"},{"location":"git/github-workflows/#version-bump","title":"Version Bump","text":"<p>The .github/workflows/version-bump.yml updates version line in <code>pyproject.toml</code> when a new release is published.</p>"},{"location":"git/github-workflows/#publish-release","title":"Publish Release","text":"<p>The .github/workflows/publish-release.yml builds the package when release is published and attaches <code>.tar.gz</code> to the assets of the release.</p>"},{"location":"git/github-workflows/#best-practices","title":"Best Practices","text":""},{"location":"git/github-workflows/#security","title":"Security","text":"<p>Protecting your workflow's security involves using secrets for sensitive data through <code>${{ secrets.SECRET_NAME }}</code> syntax, setting minimum required permissions for each job, and using specific versions for actions rather than floating references.</p>"},{"location":"git/github-workflows/#performance","title":"Performance","text":"<p>To optimize workflow performance, use Ubuntu runners for faster startup times, implement dependency caching where possible, and structure your workflows to run independent jobs in parallel.</p>"},{"location":"git/github-workflows/#maintenance","title":"Maintenance","text":"<p>Maintain a clean and organized workflow structure by keeping all workflow files in the <code>.github/workflows</code> directory. Use descriptive names for your workflows and jobs, and include clear comments to document complex steps or configurations.</p>"},{"location":"git/github-workflows/#common-pitfalls","title":"Common Pitfalls","text":"<p>When working with GitHub Workflows, watch out for common issues such as missing required permissions, using floating references like <code>@master</code> instead of specific versions, inadequate error handling, and overly complex workflow structures that are difficult to maintain.</p>"},{"location":"git/github-workflows/#related-resources","title":"Related Resources","text":"<p>For more detailed information about GitHub Actions and workflows, consult the GitHub Actions Documentation, browse available actions in the GitHub Marketplace, or refer to the Workflow Syntax Reference.</p>"},{"location":"git/intro/","title":"Git and GitHub for Computational Research","text":"<p>Git and GitHub are essential tools for both research and software development. Whether you're working on a research project or building a Python package, version control helps you track changes, collaborate with others, and maintain high-quality code. This guide shows you how to use these tools effectively in your daily work.</p>"},{"location":"git/intro/#overview","title":"Overview","text":"<p>Version control is a crucial skill for anyone writing code. You'll learn how to track your code changes, collaborate with others, and manage your projects effectively. We'll cover both research workflows and Python package development, showing you the best practices for each.</p> <p>To follow along, you'll need Git installed (version 2.0 or higher), a GitHub account, and your favorite code editor. Basic command line knowledge will help, but we'll explain all commands as we go.</p>"},{"location":"git/intro/#1-why-version-control","title":"1. Why Version Control?","text":"<p>Version control helps you in the following cases:</p>"},{"location":"git/intro/#for-research-code","title":"For Research Code","text":"<p>Your experiments generate lots of code variations and data. Git helps you track which parameters produced which results, maintain a complete history of your analysis changes, and document your computational setup. When collaborating with others, you can easily merge different analyses and review each other's work.</p> <p>When exploring new ideas, Git lets you try things out without fear of breaking what works. Every failed approach becomes valuable documentation rather than lost work. Years later, you can understand why certain decisions were made and build upon past knowledge.</p> <p>For example, imagine you have this working function for data processing:</p> <pre><code>def process_data(data):\n    \"\"\"Process input data using moving average.\"\"\"\n    window_size = 5\n    result = []\n    for i in range(len(data) - window_size + 1):\n        window = data[i : i + window_size]\n        result.append(sum(window) / window_size)\n    return result\n</code></pre> <p>Without version control, you might try something like this:</p> <pre><code>def process_data(data):\n    \"\"\"Process input data using moving average.\"\"\"\n    window_size = 5\n    result = []\n    for i in range(len(data) - window_size + 1):\n        window = data[i : i + window_size]\n        result.append(sum(window) / window_size)\n    return result\n\n\n# New version with weighted average\n# TODO: Delete old version if this works better\ndef process_data_weighted(data):\n    \"\"\"Process input data using weighted moving average.\"\"\"\n    window_size = 5\n    weights = [0.1, 0.2, 0.4, 0.2, 0.1]\n    result = []\n    for i in range(len(data) - window_size + 1):\n        window = data[i : i + window_size]\n        result.append(sum(w * x for w, x in zip(weights, window)))\n    return result\n</code></pre> <p>With Git, you can explore this change more cleanly:</p> <pre><code>git checkout -b feature/weighted-average\n# Edit process_data() directly in your code\ngit commit -m \"Switch to weighted moving average for better noise handling\"\n</code></pre> <p>If the new approach works better, merge it to main. If not, just switch back:</p> <pre><code>git checkout main  # Your original version is safe here\n</code></pre> <p>The failed experiment isn't wasted\u2014the branch preserves the complete context of what you tried and why.</p>"},{"location":"git/intro/#for-python-packages","title":"For Python Packages","text":"<p>When building Python packages, Git helps you manage releases and maintain backward compatibility. You can develop new features without disrupting users of the stable version, track bugs across releases, and coordinate with contributors worldwide.</p>"},{"location":"git/intro/#2-git-fundamentals","title":"2. Git Fundamentals","text":""},{"location":"git/intro/#21-core-concepts","title":"2.1 Core Concepts","text":"<p>Git tracks your code's evolution through a series of snapshots. Each snapshot, called a commit, records the complete state of your files at a specific point in time. You can always return to any previous state.</p> <p>Branches let you maintain different versions of your code simultaneously. The main branch typically contains your stable code, while feature branches let you experiment without affecting that stability. You can create as many branches as you need and merge them back together later.</p> <p>Tags are permanent markers you attach to specific commits. Unlike branches, tags don't move as you add new commits. They're perfect for marking important points in your project's history, like release versions or significant milestones.</p> <p>The <code>.gitignore</code> file tells Git which files it shouldn't track. This is crucial for keeping your repository clean and efficient\u2014you don't want to track temporary files, build outputs, or large data files (<code>.csv,.pkl,.ckpt</code>) that can be regenerated.</p> <p>These concepts map naturally to both research and software development:</p> Git Concept Research Use Package Development Use Commits Snapshot experiment state Record feature implementation Branches Test different approaches Develop features independently Tags Mark dataset versions Release package versions .gitignore Exclude raw data Ignore build artifacts"},{"location":"git/intro/#22-project-structure","title":"2.2 Project Structure","text":"<p>A well-structured Python project, whether for research or distribution, typically looks like this:</p> <pre><code>project/\n\u251c\u2500\u2500 src/                 # Source code\n\u2502   \u2514\u2500\u2500 yourpackage/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 core.py\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 docs/               # Documentation\n\u251c\u2500\u2500 pyproject.toml     # Dependencies\n\u251c\u2500\u2500 uv.lock           # Dependencies\n\u2514\u2500\u2500 README.md         # Project overview\n</code></pre> <p>For research projects, you might add:</p> <pre><code>research-project/\n\u251c\u2500\u2500 experiments/        # Experiment code\n\u251c\u2500\u2500 data/              # Data management\n\u2514\u2500\u2500 results/           # Outputs and figures\n</code></pre>"},{"location":"git/intro/#3-branching-strategy","title":"3. Branching Strategy","text":""},{"location":"git/intro/#31-core-branches","title":"3.1 Core Branches","text":"<p>Your main branch represents the stable, published state of your work. For research, this means verified results and polished analysis. For Python packages, it's your latest stable release.</p> <p>Create feature branches for new work. Use descriptive names like <code>experiment/neural-net</code> for research or <code>feature/async-processing</code> for package development. This keeps your changes isolated and makes it easy to track different lines of work.</p>"},{"location":"git/intro/#32-branch-lifecycle","title":"3.2 Branch Lifecycle","text":"<p>When you start new work, branch off from main. Work on your changes, commit regularly, and push to GitHub to back up your work. Once you're satisfied with the results, create a pull request for team review. After approval, merge your changes back to main.</p> <p>If an experiment or feature doesn't work out, don't delete the branch immediately. Instead, tag it (like <code>archive/failed-approach-1</code>) to preserve the knowledge of what didn't work and why.</p>"},{"location":"git/intro/#4-essential-git-commands","title":"4. Essential Git Commands","text":""},{"location":"git/intro/#common-workflows","title":"Common Workflows","text":"<p>Starting a new project:</p> <pre><code>git init                                    # Create new repository\ngit remote add origin &lt;url&gt;                 # Connect to GitHub\ngit checkout -b feature/initial-setup       # Start work on first feature\n</code></pre> <p>Tracking your changes:</p> <pre><code>git add .                                   # Stage all changes\ngit commit -m \"Add data processing module\"  # Record changes\ngit push origin feature/data-processing     # Share with team\n</code></pre> <p>Collaborating with others:</p> <p>There are two main ways to get changes from your team. Using <code>pull</code> is simpler but less precise, while <code>fetch + rebase</code> gives you more control:</p> <pre><code># Option 1: Simple but less control\ngit pull                                    # Fetch + merge in one step\n\n# Option 2: More explicit and clean history\ngit fetch                                   # Download changes without applying\ngit rebase origin/main                      # Replay your work on top of team's changes\n</code></pre> <p>The key difference is how your changes combine with your team's work:</p> <ul> <li><code>git pull</code> downloads and merges changes immediately, which can create merge commits</li> <li><code>git fetch</code> only downloads changes, letting you decide how to integrate them</li> <li><code>git rebase</code> replays your changes on top of the team's work, keeping history linear</li> </ul> <p>Choose <code>fetch + rebase</code> when you want a clean, linear history. Use <code>pull</code> for quick updates when the extra merge commits don't matter.</p>"},{"location":"git/intro/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"git/intro/#finding-bugs-with-git-bisect","title":"Finding Bugs with git bisect","text":"<p><code>git bisect</code> is a powerful debugging tool that helps you find which commit introduced a bug using binary search. You start by telling Git two points in your project's history: a \"good\" commit where everything worked and a \"bad\" commit where the bug exists. Git then walks you through the commits between these points, efficiently narrowing down where the problem started.</p> <p>The process is interactive. For each commit Git checks out, you test your code to see if the bug exists. This could mean running your test suite, checking if your API returns correct results, or verifying your ML model's accuracy. You then tell Git whether this commit is \"good\" or \"bad\", and it uses this information to choose the next commit to test. Through this binary search process, Git can find the exact commit that introduced the problem in just a few steps, even if there are hundreds of commits to search through.</p> <p>For example, imagine your machine learning model's accuracy dropped from 90% to 60%. You know it worked well last month but can't pinpoint what change caused the regression. With bisect, you'd mark last month's commit as \"good\" and the current state as \"bad\". Git would then help you systematically test commits in between, perhaps revealing that a seemingly innocent change to your data preprocessing pipeline three weeks ago actually caused the accuracy drop.</p> <pre><code>git bisect start\ngit bisect bad HEAD\ngit bisect good v1.0\n# Git will now guide you through the binary search\n</code></pre> <p>When bisect finishes, it shows you the exact commit that introduced the problem, complete with the commit message, author, and date. This information is invaluable for understanding not just what broke your code, but why and who might have context about the change.</p> <p>&lt;!-- #### Managing Large Files with Git LFS</p> <p>Git LFS (Large File Storage) solves the problem of storing large files (like datasets, models, or images) in your repository. Instead of storing the actual files in Git, LFS replaces them with small pointer files and stores the large files on a separate server.</p> <p>Here's how to use it:</p> <pre><code># Install Git LFS\ngit lfs install\n\n# Tell LFS which files to track\ngit lfs track \"*.csv\"          # Track all CSV files\ngit lfs track \"*.pkl\"          # Track all pickle files\ngit lfs track \"models/**/*.h5\" # Track all HDF5 files in models directory\n\n# The tracking rules are stored in .gitattributes\ngit add .gitattributes\ngit commit -m \"Configure Git LFS tracking\"\n\n# Now use Git normally - LFS handles the large files automatically\ngit add my_large_dataset.csv\ngit commit -m \"Add training dataset\"\n</code></pre> <p>LFS is particularly useful for: - Large datasets that need versioning - Trained model checkpoints - High-resolution images or videos - Any file over 50MB (GitHub's file size limit)</p> <p>When someone clones your repository, they can choose to download the large files or not: <pre><code>git clone --no-checkout your-repo   # Clone without downloading LFS files\ngit lfs pull                        # Download LFS files when needed\n``` --&gt;\n\n## 5. Working with GitHub\n\n### Code Review\n\nCode review is essential for both research and package development. Create a pull request when you're ready to merge your changes. Describe what you've done and what problem it solves. For research code, include key results or visualizations. For package features, explain the API changes and include usage examples.\n\n### Project Management\n\nUse GitHub Issues to track work. For research, create issues for each experiment or analysis task. For package development, track feature requests, bug reports, and improvements. Link related pull requests to automatically close issues when work is merged.\n\n### Continuous Integration\n\nSet up GitHub Actions to automate checks:\n\n```yaml\nname: CI\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Run checks\n        run: |\n          uv pip install -e \".[test]\"\n          pytest\n          ruff check\n          ruff format --check\n          mypy .\n</code></pre></p>"},{"location":"git/intro/#6-security-and-data-management","title":"6. Security and Data Management","text":""},{"location":"git/intro/#sensitive-data","title":"Sensitive Data","text":"<p>Never commit sensitive data or credentials. Use environment variables for secrets and add sensitive file patterns to .gitignore. For research data that can't be public, use Git-crypt or similar tools to encrypt specific files.</p>"},{"location":"git/intro/#access-control","title":"Access Control","text":"<p>Configure branch protection on main to require reviews and passing tests before merging. This prevents accidental changes to your stable code. For private repositories, carefully manage team access levels and regularly audit who has access.</p>"},{"location":"git/intro/#7-best-practices","title":"7. Best Practices","text":""},{"location":"git/intro/#for-research-projects","title":"For Research Projects","text":"<p>Write clear commit messages explaining what changed and why. Include enough detail to understand the experiment parameters and key results. Tag important states of your analysis for easy reference later.</p> <p>Keep your analysis reproducible by managing dependencies carefully. Use virtual environments and record exact package versions. Document setup steps clearly in your README.</p>"},{"location":"git/intro/#for-python-packages_1","title":"For Python Packages","text":"<p>Follow semantic versioning for releases. Major version changes (1.0 to 2.0) indicate breaking changes, minor versions (1.1 to 1.2) add features, and patches (1.1.1 to 1.1.2) fix bugs.</p> <p>Write comprehensive tests and maintain good documentation. Set up automated builds to catch issues early. Keep a detailed changelog to help users understand what's changed between versions.</p>"},{"location":"git/intro/#8-common-mistakes-to-avoid","title":"8. Common Mistakes to Avoid","text":"<p>Committing large data files directly to Git will bloat your repository. Instead, use Git LFS or external data storage. Store only processed, minimal datasets in Git when necessary.</p> <p>Don't make huge, sweeping changes in a single commit. Break work into logical, reviewable chunks. Each commit should be focused and self-contained.</p> <p>Avoid committing temporary files, build artifacts, or environment-specific settings. Keep your repository clean and focused on source code and essential documentation.</p>"},{"location":"git/intro/#9-additional-resources","title":"9. Additional Resources","text":"<p>The Turing Way provides excellent guidance for reproducible research. For Python packaging, the Python Packaging User Guide is the authoritative resource.</p> <p>Learn more about Git with Pro Git and GitHub's documentation. For research-specific workflows, check out Git for Scientists.</p>"},{"location":"git/pre-commit/","title":"Pre-commit Workflows","text":"<p>After reading docs on linting &amp; formatting and typing, your committing workflow will look like:</p> <pre><code>ruff check --fix\nruff format\nmypy .\ngit add .\ngit commit -m \"feat: some new feature\"\n</code></pre> <p>this might get tiresome, and it doesn't have to be that way. Enter pre-commit hooks - defined workflow that run when you try to commit something. With a <code>.pre-commit-config.yaml</code> (our template has one) you can have ruff &amp; mypy executed automatically when you make a commit:</p> <pre><code>git add .\ngit commit -m \"feat: some new feature\"\n</code></pre> <p>if the pre-commit hook will change formatting (and so the pre-commit check will fail), you'll have to run the two commands above again. Done.</p>"},{"location":"git/pre-commit/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Make sure pre-commit are installed</p> <pre><code>uv add pre-commit\n</code></pre> </li> <li> <p>Install the hooks:</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"git/pre-commit/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li>Hook fails to install: Ensure Python 3.12 is available in your environment</li> <li>Ruff errors: Run <code>ruff check --fix</code> manually to see detailed errors</li> <li>MyPy errors: Add type annotations to resolve strict type checking issues</li> </ul>"},{"location":"git/pre-commit/#best-practices","title":"Best Practices","text":"<ol> <li>Run <code>pre-commit run --all-files</code> after installing to verify setup</li> <li>Commit frequently to catch issues early</li> <li>Update hooks regularly with <code>pre-commit autoupdate</code></li> </ol>"},{"location":"git/pre-commit/#related-resources","title":"Related Resources","text":"<ul> <li>Pre-commit Documentation</li> <li>Ruff Documentation</li> <li>MyPy Documentation</li> </ul>"},{"location":"python/essential-tools/","title":"Python Starter Pack","text":""},{"location":"python/essential-tools/#overview","title":"Overview","text":"<p>This guide provides a modern, efficient approach to Python project management using <code>uv</code>, a high-performance replacement for traditional tools like pip, pyenv, and poetry. It's designed for Python developers who want to streamline their development workflow and adopt current best practices for dependency management.</p> <p>Prerequisites</p> <ul> <li>Basic familiarity with Python and command line</li> <li>Basic understanding of virtual environments and package management</li> <li>macOS, Linux, or Windows system</li> </ul>"},{"location":"python/essential-tools/#choosing-uv-as-a-package-manager","title":"Choosing uv as a package manager","text":"<p>uv is a python project and package manager that replaces <code>pip</code>, <code>pyenv</code>, <code>poetry</code>, and mostly <code>conda</code>. It's faster, simpler, and better.</p>"},{"location":"python/essential-tools/#why-not-conda","title":"Why not conda?","text":"<p>You can have many anti-conda rants on the internet. It's bulky, it has complicated installation process that easily tricks you into installing the full anaconda suit with all the things you never need. Pragmatically, each time you create a new virtual environment, it installs a new python in that env, so if you have 5 different projects each using python 3.12.8, you will have 5 copies of python 3.12.8 installed. Why would you want that?</p>"},{"location":"python/essential-tools/#why-not-pyenv-venv","title":"Why not pyenv + venv?","text":"<p>A more reasonable alternative, until Feb 2024, was using pyenv, which installs global versions of python that you can easily switch between, and then creating <code>venv</code> using python native <code>python -m venv venv</code>. Your workflow was:</p> <pre><code>pyenv local 3.12\npython -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"python/essential-tools/#so-why-uv","title":"So why uv?","text":"<p>uv basically mirrors workflow above, but makes everything faster.</p> <pre><code>uv venv --python 3.12\nsource .venv/bin/activate\n</code></pre> <p>As a bonus, installing uv is incredibly easy, just one curl script that is easily found in docs without requiring you to go through 10 accordeons and tabs.</p>"},{"location":"python/essential-tools/#starting-a-new-project","title":"Starting a new project","text":"<p>After creating and <code>cd</code> into your project directory, you can run <code>uv init</code>, which will create:</p> <ul> <li><code>.python-version</code> - a simple text file specifying python version you chose when running <code>uv venv --python 3.12</code>. If a project has <code>.python-version</code> specified, you can simply run <code>uv venv</code> and it'll use (or install if that python is not installed already) python version specified in <code>.python-version</code>.</li> <li><code>pyproject.toml</code> - a modern replacement to <code>setup.py</code>, <code>requirements.txt</code>.</li> <li><code>main.py</code> - your first <code>.py</code> file.</li> </ul> <p>See python-template repo for an example of a filled <code>pyproject.toml</code>.</p>"},{"location":"python/essential-tools/#creating-your-package","title":"Creating your package","text":"<p>Choosing when to structure your code as a package is ultimately a matter of preference. As of today, I prefer to create a package from the very beginning. Simply because:</p> <ul> <li>if it's a research project, I will release the code regardless of whether it truly is a standalone package (i.e. other people will use it in their projects as a dependency) just for reproducibility reasons</li> <li>given that <code>uv</code> uses <code>pyproject.toml</code> to enumerate dependencies already, it's a matter of a few extra lines to convert codebase into a package</li> <li>having a central package removes the necessity to either explicitly add path to sys or use some complicated relative path imports. If you had these issues before, you know what I mean.</li> </ul>"},{"location":"python/essential-tools/#why-should-i-put-my-package-under-src","title":"Why should I put my package under src/","text":"<p>You might notice that python-template repo or DirectMultiStep hides the package files under <code>src/</code> directory. This is a good practice, because, if you have the following structure:</p> <pre><code>my-project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 my_package/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 script.py\n</code></pre> <p>when you install <code>my_package</code> in development mode (e.g. <code>uv pip install -e .</code>), the project root directory (<code>my-project/</code>) is added to the python path. This means that any file in your project root can be accidentally imported as part of your package. For example, if you have a <code>script.py</code> in your project root, it could be mistakenly imported as <code>my_package.test</code>.</p> <p>By placing your package under <code>src/</code>, like this:</p> <pre><code>my-project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_package/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 tests/\n</code></pre> <p>and configuring your <code>pyproject.toml</code> accordingly, only the <code>src/</code> directory is added to the python path during development installation. This clearly separates your package code from other project files (like configuration, tests, documentation, etc.) and prevents potential naming conflicts and accidental imports from the project root. It's a cleaner and more robust way to structure your project.</p>"},{"location":"python/essential-tools/#installing-dependencies","title":"Installing Dependencies","text":""},{"location":"python/essential-tools/#your-package","title":"Your package","text":"<p>Once you've created some package in <code>src/</code>, all that remains is to add this to your <code>pyproject.toml</code>:</p> <pre><code>[tool.setuptools]\npackages = [\"my_package\"]\npackage-dir = { \"\" = \"src\" }\n</code></pre> <p>Don't forget to activate your venv</p> <p>Once you have your project set up, your venv should be pretty much always activated. Your IDE (VSCode, Cursor) should automatically recognize presence of <code>.venv</code> folder and activate it in the terminal sessions. But if you open your repository in a standalone terminal window, you'll have to manually <code>source .venv/bin/activate</code>.</p> <p>After this, you can install your package in development mode <code>uv pip install -e .</code>. The <code>-e</code> specifies dev mode and effectively it means that any changes made to the source code in <code>src/my_package</code> will be immediately available (i.e. you don't need to reinstall your package).</p>"},{"location":"python/essential-tools/#external-dependencies","title":"External Dependencies","text":"<p>Instead of running <code>pip install package</code> you should use <code>uv add package</code>. Using <code>uv add</code> instead of <code>pip install</code> results in:</p> <ul> <li><code>package</code> added as dependency to <code>pyproject.toml</code></li> <li><code>uv.lock</code> updated</li> </ul> <p>You might find uv's docs helpful if you need to specify specific (including platform-specific) source for the package.</p>"},{"location":"python/essential-tools/#pyprojecttoml-and-uvlock","title":"pyproject.toml and uv.lock","text":"<p>At this point, you might wonder -- what is <code>uv.lock</code> and how is it different from <code>pyproject.toml</code>?</p> <p>In short:</p> <ul> <li><code>pyproject.toml</code> is a soft specification of minimal version requirements of top-level packages</li> <li><code>uv.lock</code> is a strict and full specification of all packages installed in current venv.</li> </ul> <p>As an example, if you <code>uv add</code> a package, say, SciPy, <code>pyproject.toml</code> will add <code>scipy&gt;=1.15.2.</code>, i.e. a spec of minimally required version. However, SciPy itself depends on NumPy, so NumPy will be installed in the venv and reflected in <code>uv.lock</code>, but not in <code>pyproject.toml</code>.</p> <p>In some sense, <code>uv.lock</code> is similar to what you'd get from <code>pip freeze</code>. Why do we need both <code>pyproject.toml</code> and <code>uv.lock</code>?</p> <ul> <li><code>pyproject.toml</code> is useful to keep track of which packages you intentionally added. If you want to update, say, SciPy to 1.16, you can update <code>pyproject.toml</code> and run <code>uv lock --upgrade</code>. If SciPy depends on 10 other packages, you shouldn't be required to manually update dependencies of all those packages, which is why it's all handled for you by <code>uv.lock</code>.</li> <li><code>uv.lock</code> is useful because it allows any other user to recreate your local environment exactly. You never need to manually modify this file, as long as you use <code>uv add</code>, <code>uv remove</code> to handle dependencies.</li> </ul> <p>See uv locking and syncing page for more details.</p> <p>uv.lock gives you confidence that your code will always work</p> <p>Basically, once you have a working codebase, even if you stop working on it, you (or any other human) can come back to it at any point in the future, and he'll be able to run it without any issues. Your code will work even if numpy updated 10 times in the meantime or some other package stopped being maintained. Fun fact: lock files have been a standard in web dev for almost 15 years, you might be familiar with <code>package-lock.json</code> and <code>yarn.lock</code>.</p>"},{"location":"python/essential-tools/#dependency-groups","title":"Dependency Groups","text":"<p>By default, every package you <code>uv add</code> appears as:</p> <pre><code>[project]\ndependencies = [\"numpy==1.26.4\"]\n</code></pre> <p>However, you can also have optional dependency groups:</p> <pre><code>[project.optional-dependencies]\ndev = [\"ruff&gt;=0.9.6\", \"mypy&gt;=1.15.0\"]\nweb = [\"flask&gt;=3.1.0\"]\n</code></pre> <p>which you can install by:</p> <pre><code>uv pip install -e \".[dev]\" # will install main deps and dev deps\nuv pip install -e \".[web]\" # will install main + web deps\nuv pip install -e \".[dev,web]\" # will install main + dev + web deps\n</code></pre> <p>which is useful when, say, 2 people are working on the project and only one of them develops the web-facing code. There's no reason for the other person to have all web-related dependencies in his venv, so they can be separated into a dep group.</p>"},{"location":"python/essential-tools/#migrating-from-traditional-tools","title":"Migrating from Traditional Tools","text":""},{"location":"python/essential-tools/#from-pip-venv","title":"From pip + venv","text":"<p>If you're currently using pip with venv, migration is straightforward:</p> <ol> <li> <p>Install uv: Follow the installation instructions above</p> </li> <li> <p>For existing projects:</p> <pre><code># In your project directory\nuv pip freeze &gt; requirements.txt  # Export current dependencies\ndeactivate  # Exit current venv\nrm -rf venv/  # Remove old venv\nuv venv  # Create new venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt  # Install dependencies\n</code></pre> </li> <li> <p>Convert to modern structure:</p> <ul> <li>Create <code>pyproject.toml</code> using <code>uv init</code></li> <li>Move dependencies from <code>requirements.txt</code> to <code>pyproject.toml</code></li> <li>Run <code>uv pip install -e .</code> to install in editable mode</li> </ul> </li> </ol>"},{"location":"python/essential-tools/#from-conda","title":"From conda","text":"<p>For conda users, the transition requires a few additional steps:</p> <ol> <li> <p>Export your conda environment:</p> <pre><code>conda list --explicit &gt; conda-packages.txt\n</code></pre> </li> <li> <p>Identify pure Python packages from conda-packages.txt</p> </li> <li> <p>Create new project with uv:</p> <pre><code>uv venv\nsource .venv/bin/activate\n</code></pre> </li> <li> <p>Install required packages using <code>uv add</code></p> </li> <li> <p>For conda-specific packages (like MKL-optimized numpy), refer to package documentation for pip-compatible alternatives</p> </li> </ol>"},{"location":"python/essential-tools/#summary","title":"Summary","text":""},{"location":"python/essential-tools/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>uv</code> provides a faster, simpler alternative to traditional Python tooling</li> <li>The src/ layout with pyproject.toml offers a clean, modern project structure</li> <li>Lock files ensure reproducible environments across team members and time</li> <li>Dependency groups help manage optional features efficiently</li> </ul>"},{"location":"python/essential-tools/#related-resources","title":"Related Resources","text":"<ul> <li>uv Documentation</li> <li>Python Packaging User Guide</li> <li>Batista Group Python Template</li> </ul>"},{"location":"python/lint/","title":"Python Code Linting with Ruff","text":""},{"location":"python/lint/#overview","title":"Overview","text":"<p>Ruff is a fast, Python-native code linter that helps maintain code quality and consistency across your Python projects. It replaces multiple traditional Python linters (like flake8, pylint, isort) with a single, blazingly fast tool written in Rust.</p>"},{"location":"python/lint/#why-ruff","title":"Why Ruff?","text":"<ul> <li>Speed: Up to 100x faster than traditional Python linters</li> <li>All-in-One: Combines functionality of multiple linters in a single tool</li> <li>Extensible: Rich set of rules and easy configuration</li> <li>Auto-fixing: Can automatically fix many common issues</li> <li>Modern: Supports the latest Python features and best practices</li> </ul>"},{"location":"python/lint/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>A Python project with <code>pyproject.toml</code> configuration</li> </ul>"},{"location":"python/lint/#installation","title":"Installation","text":"<pre><code>uv add ruff --dev\n</code></pre>"},{"location":"python/lint/#configuration","title":"Configuration","text":"<p>Our template project uses Ruff with the following configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\nline-length = 120\nlint.select = [\"E\", \"F\", \"UP\", \"B\", \"SIM\", \"I\"]\n</code></pre>"},{"location":"python/lint/#rule-sets-explained","title":"Rule Sets Explained","text":"<ol> <li> <p>E (pycodestyle)</p> <ul> <li>Enforces PEP 8 style guide</li> <li>Handles spacing, naming, and basic code style</li> </ul> </li> <li> <p>F (Pyflakes)</p> <ul> <li>Detects logical errors</li> <li>Finds unused imports and variables</li> <li>Identifies undefined names</li> </ul> </li> <li> <p>UP (pyupgrade)</p> <ul> <li>Upgrades syntax to newer Python versions</li> <li>Modernizes code constructs</li> </ul> </li> <li> <p>B (flake8-bugbear)</p> <ul> <li>Catches common bugs and design problems</li> <li>Enforces best practices</li> </ul> </li> <li> <p>SIM (flake8-simplify)</p> <ul> <li>Suggests code simplifications</li> <li>Improves code readability</li> </ul> </li> <li> <p>I (isort)</p> <ul> <li>Sorts and organizes imports</li> <li>Maintains consistent import ordering</li> </ul> </li> </ol>"},{"location":"python/lint/#usage","title":"Usage","text":""},{"location":"python/lint/#basic-commands","title":"Basic Commands","text":"<ol> <li> <p>Check your code:</p> <pre><code>ruff check .\n</code></pre> </li> <li> <p>Automatically fix issues:</p> <pre><code>ruff check --fix .\n</code></pre> </li> </ol>"},{"location":"python/lint/#integration-with-development-tools","title":"Integration with Development Tools","text":"<p>See page on pre-commits.</p>"},{"location":"python/lint/#ruff-formatting","title":"Ruff Formatting","text":"<p>Perhaps one of the first effects of ruff you'll notice is that it'll break long lines. By default, Ruff follows Black's config and limits number of characters to 88, I personally extend it to 120. This change might feel very counterintuitive to you (you'll see what I mean in practice), but if you trust it for awhile, you'll start seeing the benefits. A few examples.</p> <pre><code># Example 1: Complex Boolean Logic\n# Bad - Hard to understand the logic and spot errors\nif user.is_active and user.email_verified and (user.role == \"admin\" or user.role == \"moderator\") and user.has_permission(\"edit_posts\") and not user.is_banned:\n    allow_access()\n\n# Good - Logic is clear and errors are easy to spot\nif (\n    user.is_active\n    and user.email_verified\n    and user.role in (\"admin\", \"moderator\")\n    and user.has_permission(\"edit_posts\")\n    and not user.is_banned\n):\n    allow_access()\n\n# Example 2: Error Handling\n# Bad - Exception types are hard to read and modify\ntry:\n    process_data()\nexcept (ValueError, TypeError, KeyError, DatabaseError, NetworkTimeout, ValidationError) as e:\n    log_error(e)\n\n# Good - Each exception is clear and git diffs will show exactly what changed\ntry:\n    process_data()\nexcept (\n    ValueError,\n    TypeError,\n    KeyError,\n    DatabaseError,\n    NetworkTimeout,\n    ValidationError,\n) as e:\n    log_error(e)\n\n# Example 3: Function Calls with Named Parameters\n# Bad - Parameter names and values are hard to scan\ncreate_user(username=\"johndoe\", email=\"john@example.com\", role=\"admin\", department=\"engineering\", active=True, send_welcome_email=True)\n\n# Good - Each parameter is clearly visible and self-documenting\ncreate_user(\n    username=\"johndoe\",\n    email=\"john@example.com\",\n    role=\"admin\",\n    department=\"engineering\",\n    active=True,\n    send_welcome_email=True,\n)\n</code></pre> <p>The benefits of proper line formatting become obvious when you need to:</p> <ul> <li>Debug complex boolean conditions</li> <li>Review which exceptions are being caught</li> <li>Understand the parameters being passed to a function</li> <li>Track changes in git history (each parameter change appears on its own line)</li> </ul> <p>Note the trailing comma after the last item in multi-line structures - this is a deliberate practice that makes future modifications cleaner in version control and prevents syntax errors when reordering lines.</p>"},{"location":"python/lint/#best-practices","title":"Best Practices","text":"<ol> <li>Run Ruff locally before committing changes</li> <li>Configure your IDE for automatic linting on save</li> <li>Use pre-commit hooks to enforce linting rules</li> <li>Regularly update Ruff to get the latest rules and fixes</li> </ol>"},{"location":"python/lint/#related-resources","title":"Related Resources","text":"<ul> <li>Ruff Documentation</li> <li>PEP 8 Style Guide</li> <li>Pre-commit Hooks</li> </ul>"},{"location":"python/template-repo/","title":"Python Project Template","text":"<p>Setting proper project structure, configuring <code>pyproject.toml</code>, writing pre-commit config, configuring github workflows is quite time consuming, but most importantly pretty much doesn't change from one project to another.</p> <p>Which is why we created a boilerplate python template that is already configured with many best practices in mind.</p>"},{"location":"python/tests/intro/","title":"The Art of Testing Python Code","text":"<p>Testing transforms your code from a fragile house of cards into a robust, maintainable fortress by systematically validating its behavior and documenting its intent.</p> <p>Admittedly, testing is not something I've fully embraced, I don't have (yet) a single codebase that has 90+% test coverage; however, the more I've worked on projects where more than one person writes the code, the more I understand their necessity. At the end of the day, just like with typing, it's something you need to force yourself to do once, and after that it will become a natural part of coding.</p>"},{"location":"python/tests/intro/#testing-in-the-wild","title":"Testing in the wild","text":"<p>Testing is pretty much an unquestionable industry standard. Any codebase that will be reused strives to have code coverage of at least 90% (meaning that 90% of lines of code have a test case explicitly testing them).</p> <p>Perhaps the most famous example of a well tested codebase is SQLite, a SQL based db used by Adobe, Google (Android and Chrome), Apple (iOS), Dropbox, and many other giants. SQLite library contains 155 800 lines of source code in C (excluding blanks and comments). The test suite for SQLite has 92 053 100 lines of code, i.e. 590 lines of testing for every line of code.</p> <p>The reliability and robustness of SQLite is achieved in part by thorough and careful testing.[source]</p> <p>Python code can be tested using pytest library. The rest of this page documents basic use.</p>"},{"location":"python/tests/intro/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>Understanding of Python functions and classes</li> </ul> <pre><code>source .venv/bin/activate\nuv add pytest typing-extensions mypy\n</code></pre>"},{"location":"python/tests/intro/#testing-fundamentals","title":"Testing Fundamentals","text":""},{"location":"python/tests/intro/#1-unit-tests-the-foundation","title":"1. Unit Tests: The Foundation","text":"<p>Unit tests validate individual components in isolation. They are your first line of defense.</p> <pre><code>from typing import Any\nfrom decimal import Decimal\n\n\nclass PricingEngine:\n    \"\"\"Handles product pricing calculations.\"\"\"\n\n    def calculate_discount(self, price: Decimal, percentage: Decimal) -&gt; Decimal:\n        \"\"\"Calculate discounted price.\n\n        Args:\n            price: Original price\n            percentage: Discount percentage (0-100)\n\n        Returns:\n            Discounted price\n\n        Raises:\n            ValueError: If percentage is not between 0 and 100\n        \"\"\"\n        if not 0 &lt;= percentage &lt;= 100:\n            raise ValueError(\"Percentage must be between 0 and 100\")\n        return price * (1 - percentage / 100)\n\n\ndef test_pricing_engine() -&gt; None:\n    \"\"\"Demonstrate comprehensive unit testing.\"\"\"\n    engine = PricingEngine()\n\n    # Happy path\n    assert engine.calculate_discount(Decimal(\"100.00\"), Decimal(\"20.00\")) == Decimal(\n        \"80.00\"\n    )\n\n    # Edge cases\n    assert engine.calculate_discount(Decimal(\"100.00\"), Decimal(\"0.00\")) == Decimal(\n        \"100.00\"\n    )\n    assert engine.calculate_discount(Decimal(\"100.00\"), Decimal(\"100.00\")) == Decimal(\n        \"0.00\"\n    )\n\n    # Error cases\n    try:\n        engine.calculate_discount(Decimal(\"100.00\"), Decimal(\"101.00\"))\n        assert False, \"Should raise ValueError\"\n    except ValueError as e:\n        assert str(e) == \"Percentage must be between 0 and 100\"\n</code></pre>"},{"location":"python/tests/intro/#2-integration-tests-component-harmony","title":"2. Integration Tests: Component Harmony","text":"<p>Integration tests verify that components work together correctly. They catch interface mismatches and data flow issues.</p> <pre><code>from typing import Optional\nfrom dataclasses import dataclass\nfrom decimal import Decimal\n\n\n@dataclass\nclass Product:\n    id: str\n    name: str\n    price: Decimal\n\n\nclass ProductDatabase:\n    def __init__(self) -&gt; None:\n        self._products: dict[str, Product] = {}\n\n    def add(self, product: Product) -&gt; None:\n        self._products[product.id] = product\n\n    def get(self, product_id: str) -&gt; Optional[Product]:\n        return self._products.get(product_id)\n\n\nclass PricingService:\n    def __init__(self, db: ProductDatabase, engine: PricingEngine) -&gt; None:\n        self.db = db\n        self.engine = engine\n\n    def apply_discount(self, product_id: str, discount: Decimal) -&gt; Optional[Product]:\n        \"\"\"Apply discount to product price.\n\n        Args:\n            product_id: Product identifier\n            discount: Discount percentage\n\n        Returns:\n            Updated product or None if not found\n        \"\"\"\n        product = self.db.get(product_id)\n        if not product:\n            return None\n\n        discounted_price = self.engine.calculate_discount(product.price, discount)\n        return Product(product.id, product.name, discounted_price)\n\n\ndef test_pricing_service_integration() -&gt; None:\n    \"\"\"Demonstrate integration testing.\"\"\"\n    # Setup components\n    db = ProductDatabase()\n    engine = PricingEngine()\n    service = PricingService(db, engine)\n\n    # Prepare test data\n    original_product = Product(\"PROD1\", \"Test Product\", Decimal(\"100.00\"))\n    db.add(original_product)\n\n    # Test integrated flow\n    discounted_product = service.apply_discount(\"PROD1\", Decimal(\"20.00\"))\n    assert discounted_product is not None\n    assert discounted_product.price == Decimal(\"80.00\")\n\n    # Test error handling\n    assert service.apply_discount(\"NONEXISTENT\", Decimal(\"20.00\")) is None\n</code></pre>"},{"location":"python/tests/intro/#3-functional-tests-user-perspective","title":"3. Functional Tests: User Perspective","text":"<p>Functional tests validate complete features from a user's perspective. They ensure the system works as a whole.</p> <pre><code>from fastapi import FastAPI, HTTPException\nfrom fastapi.testclient import TestClient\nfrom decimal import Decimal\nfrom typing import Dict, Any\n\napp = FastAPI()\ndb = ProductDatabase()\nengine = PricingEngine()\nservice = PricingService(db, engine)\n\n\n@app.post(\"/products/{product_id}/discount\")\nasync def apply_discount(\n    product_id: str, discount_percentage: Decimal\n) -&gt; Dict[str, Any]:\n    \"\"\"Apply discount to product.\"\"\"\n    result = service.apply_discount(product_id, discount_percentage)\n    if not result:\n        raise HTTPException(status_code=404, detail=\"Product not found\")\n    return {\n        \"id\": result.id,\n        \"name\": result.name,\n        \"original_price\": str(result.price),\n        \"discount_percentage\": str(discount_percentage),\n        \"final_price\": str(result.price),\n    }\n\n\ndef test_discount_api() -&gt; None:\n    \"\"\"Demonstrate functional testing of the API.\"\"\"\n    client = TestClient(app)\n\n    # Setup test data\n    db.add(Product(\"PROD1\", \"Test Product\", Decimal(\"100.00\")))\n\n    # Test successful discount\n    response = client.post(\"/products/PROD1/discount?discount_percentage=20.0\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"final_price\"] == \"80.00\"\n\n    # Test error handling\n    response = client.post(\"/products/NONEXISTENT/discount?discount_percentage=20.0\")\n    assert response.status_code == 404\n</code></pre>"},{"location":"python/tests/intro/#testing-best-practices","title":"Testing Best Practices","text":"<ol> <li> <p>Test Organization</p> <ul> <li>One test file per source file</li> <li>Group related tests in classes</li> <li>Name tests descriptively</li> </ul> </li> <li> <p>Test Coverage</p> <ul> <li>Happy path: Normal operation</li> <li>Edge cases: Boundary conditions</li> <li>Error cases: Expected failures</li> <li>Security: Input validation</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Use appropriate test scopes</li> <li>Mock expensive operations</li> <li>Parallelize test execution</li> </ul> </li> </ol>"},{"location":"python/tests/intro/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Incomplete Testing</p> <pre><code># BAD: Only testing happy path\ndef test_incomplete():\n    assert calculate_discount(100, 20) == 80\n\n\n# GOOD: Testing all scenarios\ndef test_complete():\n    assert calculate_discount(100, 20) == 80  # Happy path\n    assert calculate_discount(100, 0) == 100  # Edge case\n    with pytest.raises(ValueError):  # Error case\n        calculate_discount(100, -1)\n</code></pre> </li> <li> <p>Brittle Tests</p> <pre><code># BAD: Testing implementation details\ndef test_brittle(mocker):\n    mocker.spy(service, \"_internal_calculation\")\n    service.process()\n    assert service._internal_calculation.called\n\n\n# GOOD: Testing observable behavior\ndef test_robust():\n    result = service.process()\n    assert result.status == \"success\"\n</code></pre> </li> <li> <p>Poor Isolation</p> <pre><code># BAD: Shared state between tests\ntotal = 0\n\n\ndef test_shared_state1():\n    global total\n    total += 1\n    assert total == 1\n\n\n# GOOD: Isolated tests\ndef test_isolated1():\n    calculator = Calculator()\n    assert calculator.add(1) == 1\n</code></pre> </li> </ol>"},{"location":"python/tests/intro/#resources","title":"Resources","text":"<ul> <li>pytest Documentation</li> <li>Testing Best Practices</li> <li>Property-Based Testing</li> </ul>"},{"location":"python/typing/intro/","title":"Introduction to Python Type Hints","text":""},{"location":"python/typing/intro/#overview","title":"Overview","text":"<p>Consider the following code block.</p> <pre><code>def process_smiles(smiles):\n    # a lot of lines of code\n    ...\n</code></pre> <p>You probably can infer that this function expects SMILES and performs some operations on them. But does it take one SMILES string or does it take a list of those strings? Or can it take both? Compare this to:</p> <pre><code>def process_smiles_v1(smiles:str) -&gt; float:\n    ...\n# or \ndef process_smiles_v2(smiles:list[str]) -&gt; list[int]:\n    ...\n</code></pre> <p>although same information could be stored in a docstring, a type annotation is like a shortcut. As a side bonus, if you try to write:</p> <pre><code>process_smiles_v2(\"CCO\")\n</code></pre> <p>a type checker will throw an error which will allow you to catch this bug even before you run the code.</p> <p>Extra benefits of typing</p> <p>Besides catching errors early and improving readability of your code, a huge benefit of type hints is that it improves your experience with LLMs. Higher quality codebases tend to have type annotations, so if your existing code has the type hints, it'll direct LLM into the portion of the latent space that corresponds to higher code quality (just like a math problem written with LaTeX symbols has a higher chance of getting correct result).</p> <p>Info</p> <ul> <li>Python 3.9+</li> <li>Basic Python knowledge</li> <li>A type checker (mypy, pytype, or Pyright). I tend to use mypy</li> </ul>"},{"location":"python/typing/intro/#basic-usage","title":"Basic Usage","text":""},{"location":"python/typing/intro/#1-simple-type-annotations","title":"1. Simple Type Annotations","text":"<p>Type annotations are optional when the type can be inferred from the initial value.</p> <pre><code>name = \"Alice\"  # Type inferred as str\nage = 25  # Type inferred as int\nis_active = True  # Type inferred as bool\n</code></pre> <p>However, explicit annotations are recommended for empty collections or when type is ambiguous</p> <pre><code>users: list[str] = [] # list of strings\ncache: dict[str, int] = {} # dict mapping str keys to int vals\nnumbers: list[float | int] = []    \n\n# Function annotations are always recommended for clarity\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\ndef multiply(x: int, y: int) -&gt; int:\n    return x * y\n</code></pre>"},{"location":"python/typing/intro/#2-built-in-collections","title":"2. Built-in Collections","text":"<pre><code># Lists\nnumbers: list[int] = [1, 2, 3]\n\n\ndef get_users() -&gt; list[str]:\n    return [\"Alice\", \"Bob\", \"Charlie\"]\n\n\n# Dictionaries\nscores: dict[str, int] = {\"Alice\": 95, \"Bob\": 85}\n\n\ndef get_config() -&gt; dict[str, str]:\n    return {\"host\": \"localhost\", \"port\": \"8080\"}\n\n\n# Sets and Tuples\nvalid_codes: set[int] = {200, 201, 204}\npoint: tuple[float, float] = (2.0, 3.0)\n</code></pre>"},{"location":"python/typing/intro/#3-nullable-types-and-default-values","title":"3. Nullable Types and Default Values","text":"<p>If a variable may have type A or B, a union operator <code>|</code> can be used:</p> <pre><code># Parameters that might be None\ndef find_user(user_id: str | None) -&gt; str:\n    if user_id is None:\n        return \"Guest\"\n    return f\"User {user_id}\"\n</code></pre> <p>default values can be specified after the type:</p> <pre><code>def connect(host: str = \"localhost\", port: int = 8080) -&gt; bool:\n    return True\n</code></pre> <p>Old type annotation syntax</p> <p>Before python 3.9, a union of types was shown as <code>Union[str, None]</code>. An optional value, e.g. <code>user_id: str | None = None</code> was written <code>user_id: Optional[str] = None</code> with an import <code>from typing import Union, Optional</code>. You might see this syntax in older codebases; the new one is recommended. If you're up for a rabbit hole, python core devs had a holy war over this change.</p>"},{"location":"python/typing/intro/#4-multiple-return-types","title":"4. Multiple Return Types","text":"<pre><code>def parse_value(value: str) -&gt; int | float:\n    \"\"\"Parse a string into either an integer or float.\"\"\"\n    try:\n        return int(value)\n    except ValueError:\n        return float(value)\n\n\ndef fetch_user_data(user_id: int) -&gt; dict[str, str | int] | None:\n    \"\"\"Fetch user data, returns None if user not found.\"\"\"\n    if user_id &lt; 0:\n        return None\n    return {\"name\": \"Alice\", \"age\": 25}\n</code></pre>"},{"location":"python/typing/intro/#5-type-aliases","title":"5. Type Aliases","text":"<p>Type aliases help manage complex type annotations and make code more readable:</p> <pre><code># Type aliases for complex types\nUserId = int\nUserDict = dict[UserId, str]\nJsonValue = str | int | float | bool | None\nJsonObject = dict[str, JsonValue]\n\n\ndef process_json(data: JsonObject) -&gt; list[str]:\n    \"\"\"Process a JSON object and return list of keys with string values.\"\"\"\n    return [k for k, v in data.items() if isinstance(v, str)]\n\n\ndef get_user_data() -&gt; UserDict:\n    return {1: \"Alice\", 2: \"Bob\"}\n</code></pre>"},{"location":"python/typing/intro/#type-checking-in-practice","title":"Type Checking in Practice","text":""},{"location":"python/typing/intro/#static-vs-runtime-type-checking","title":"Static vs Runtime Type Checking","text":"<p>Type hints enable static analysis of your code before execution. This creates two distinct phases of type checking:</p> <ol> <li> <p>Static Analysis (Before Runtime):</p> <ul> <li>Checks type consistency</li> <li>Validates function signatures</li> <li>Ensures type-safe operations</li> <li>No performance impact at runtime</li> </ul> </li> <li> <p>Runtime Behavior (During Execution):</p> <ul> <li>Python's normal dynamic type checking</li> <li>No overhead from type hints (they're ignored)</li> <li>Actual type enforcement</li> </ul> </li> </ol> <p>Here's how this dual system works:</p> <pre><code>def process_items(items: list[str]) -&gt; dict[str, int]:\n    \"\"\"Process a list of strings and return their lengths.\"\"\"\n    return {item: len(item) for item in items}\n\n\n# Static type checker catches these errors:\nprocess_items([1, 2, 3])  # Error: list[int] is not list[str]\nprocess_items(\"not a list\")  # Error: str is not list[str]\n\n\n# Runtime errors (not caught by static checker):\ndef load_and_process_data(filepath: str) -&gt; dict[str, int]:\n    \"\"\"Load data from a file and process it.\n\n    The type checker trusts our annotation that the file contains strings,\n    but at runtime, the file might contain any type of data.\n    \"\"\"\n    with open(filepath) as f:\n        # Type checker assumes this creates List[str] based on annotation\n        items: list[str] = f.read().splitlines()\n        return process_items(items)\n\n\n# These will pass type checking but might fail at runtime:\nload_and_process_data(\"data.txt\")\n# Runtime error if file contains non-string data or if file doesn't exist\n</code></pre> <p>The key difference is:</p> <ul> <li>Static type checking validates code structure and explicit type annotations</li> <li>Runtime checks validate actual data values and operations</li> <li>External data (files, network, user input) can't be verified by the type checker</li> </ul>"},{"location":"python/typing/intro/#common-type-checking-scenarios","title":"Common Type Checking Scenarios","text":"<p>Here are realistic examples of issues that type checkers catch:</p> <pre><code># Function argument mismatches\ndef calculate_average(numbers: list[float]) -&gt; float:\n    return sum(numbers) / len(numbers)\n\n\ndata = [\"1\", \"2\", \"3\"]  # Error: list[str] incompatible with List[float]\ncalculate_average(data)\n\n\n# Incorrect attribute access\nclass User:\n    def __init__(self, name: str):\n        self.name = name\n\n\ndef print_email(user: User) -&gt; None:\n    print(user.email)  # Error: 'User' has no attribute 'email'\n\n\n# Type narrowing errors\ndef process_value(value: str | int) -&gt; str:\n    if isinstance(value, int):\n        return value.upper()  # Error: 'int' has no attribute 'upper'\n    return value.upper()\n\n\n# Container type mismatches\ncache: dict[str, list[int]] = {\n    \"numbers\": [\"1\", \"2\", \"3\"]  # Error: list[str] incompatible with list[int]\n}\n\n\n# Function return type violations\ndef get_user_ids() -&gt; list[int]:\n    users = {\"1\": \"Alice\", \"2\": \"Bob\"}\n    return list(users.keys())  # Error: list[str] incompatible with list[int]\n</code></pre>"},{"location":"python/typing/intro/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always Annotate Public APIs</p> <ul> <li>Functions and methods exposed to other modules</li> <li>Class attributes and properties</li> <li>Module-level variables</li> </ul> </li> <li> <p>Use Type Aliases for Complex Types</p> <pre><code>from typing import TypeAlias\n\nJsonDict: TypeAlias = dict[str, str | int | float | bool | None]\n</code></pre> </li> <li> <p>Handle Optional Types Safely</p> <pre><code>def process_data(data: str | None) -&gt; str:\n    if data is None:\n        return \"No data\"\n    return data.upper()\n</code></pre> </li> <li> <p>Document Type Variables</p> <pre><code>from typing import TypeVar\n\nT = TypeVar(\"T\", str, int)  # T can be str or int\n\n\ndef first_element(lst: list[T]) -&gt; T:\n    return lst[0]\n</code></pre> </li> </ol>"},{"location":"python/typing/intro/#configuring-mypy","title":"Configuring mypy","text":"<p>Enabling an automatic type checker for your codebase is quite easy. Simply <code>uv add mypy</code> in your venv, and then you can run <code>mypy .</code>. Once your codebase satisfies basic rules (checked by <code>mypy .</code> call), you can try &amp; run <code>mypy --strict .</code>. Mypy can also be added as a pre-commit hook.</p> <p>You can also update your <code>pyproject.toml</code> (alredy done in python-template) to have strict enabled by default.</p> <pre><code>[tool.mypy]\nstrict = true\nexclude = [\"tests\"]                    # specifies which directories are not checked\nignore_missing_imports = true          # so that it doesn't complain about types in external libraries\ndisable_error_code = [\"unused-ignore\"]\n</code></pre> <p>If you start introducing types to your existing codebase, you might get quite discourage by the seeming amount of effort to fix 100s of mypy strict errors. I can only say that I personally went through that experience twice, after which I could write new code with type hints present by default, which made my code better and more reliable.</p>"},{"location":"python/typing/intro/#summary","title":"Summary","text":"<p>Type hints are a powerful tool for improving code clarity, reducing bugs, and ensuring robust software development. Whether you're working on a small script or a large-scale project, leveraging Python's type system can significantly enhance your development process.</p>"},{"location":"quantum-chemistry/edu-tutorials/","title":"Educational Tutorials","text":"<p>A comprehensive collection of quantum chemistry and computational chemistry tutorials from the Batista Group, covering various computational methods, molecular simulations, and software tools.</p>"},{"location":"quantum-chemistry/edu-tutorials/#molecular-simulation-and-dynamics","title":"Molecular Simulation and Dynamics","text":""},{"location":"quantum-chemistry/edu-tutorials/#interfacial-electron-transfer","title":"Interfacial Electron Transfer","text":"<ul> <li>Hands on Simulations of Interfacial Electron Transfer - Practical guide to simulating electron transfer at interfaces</li> <li>Aligning Fermi Levels - Supplementary tutorial on Fermi level alignment for interfacial systems</li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#quantum-dynamics","title":"Quantum Dynamics","text":"<ul> <li>Quantum Dynamics - Introduction to quantum dynamical simulations</li> <li>Time-Sliced Thawed Gaussian Propagation for Simulations of Quantum Dynamics - Video lecture on advanced quantum dynamics methods</li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#computational-methods-and-theory","title":"Computational Methods and Theory","text":""},{"location":"quantum-chemistry/edu-tutorials/#redox-chemistry","title":"Redox Chemistry","text":"<ul> <li>Tutorial on Ab Initio Redox Potential Calculations - Guide for calculating redox potentials from first principles</li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#molecular-design","title":"Molecular Design","text":"<ul> <li>Inverse Molecular Design - Tool and tutorial for inverse molecular design approaches</li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#qmmm-methods","title":"QM/MM Methods","text":"<ul> <li>Mod-QM/MM method - Information on modified QM/MM methodology</li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#software-specific-tutorials","title":"Software-Specific Tutorials","text":""},{"location":"quantum-chemistry/edu-tutorials/#dftb-methods","title":"DFTB Methods","text":"<ul> <li>DFTB with DFTB+ or Gaussian - Density functional tight-binding tutorial</li> <li>Resources:<ul> <li>DFTBscriptfiles_noexecutable.zip</li> <li>alanine_dftb_lufeng.com</li> </ul> </li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#gaussian-09","title":"Gaussian 09","text":"<ul> <li>Quick Tutorial on Natural Bond Order 3 Calculations Within Gaussian 09 - NBO analysis in Gaussian</li> <li> <p>Resources:</p> <ul> <li>allBonds.gjf</li> </ul> </li> <li> <p>Advice on Effective Core Potential (ECP) Basis Sets - Guide for selecting appropriate ECP basis sets</p> </li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#exafs-spectroscopy","title":"EXAFS Spectroscopy","text":"<ul> <li>Tutorial on Simulating EXAFS Using Demeter - Extended X-ray absorption fine structure simulations</li> <li>Resources:<ul> <li>Build_EXAFS_Oct2016.tar.gz</li> </ul> </li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#transport-and-advanced-methods","title":"Transport and Advanced Methods","text":"<ul> <li>Non-equilibrium Green's Function Calculations with TranSIESTA - Electronic transport calculations</li> <li>Resources:<ul> <li>Benzenedithiol.mol2</li> <li>Au67.mol2</li> <li>Utils.zip</li> </ul> </li> </ul>"},{"location":"quantum-chemistry/edu-tutorials/#electron-transfer-theory","title":"Electron Transfer Theory","text":"<ul> <li>Marcus Theory with Gaussian and ADF - Classical electron transfer theory calculations</li> <li>Resources:<ul> <li>MarcusFiles.zip</li> </ul> </li> </ul>"}]}